{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ajuste hyperparametros RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "\n",
    "from funcionesComunes import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bucle de ajuste de hiperparametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for archivo in os.listdir(\"../Datasets/RCPMerged/\"):\n",
    "    print(f\"Procesando archivo: {os.path.splitext(archivo)[0]}\")\n",
    "\n",
    "    # Leemos el archivo de pindrow y eleiminamos la poblacion INDI005\n",
    "    df = pd.read_csv(f\"../Datasets/RCPMerged/{archivo}\")\n",
    "    df = df[~df[\"nametag\"].str.startswith(\"INDI005\")]\n",
    "\n",
    "    df = codification(df)\n",
    "\n",
    "    # Ejecutamos el cambio de la estructura de datos para el modelo incluyendo el clima simulado\n",
    "    df = dataStructureSimulatedClimate(df)\n",
    "\n",
    "    # Normalizamos los datos de crecimiento de los individuos\n",
    "    df, valorNormalizacion = individualNormalization(df)\n",
    "\n",
    "    train_data, val_data, test_data = split_population_individuals(df, train_pct=0.80, val_pct_in_train=0.20, details=False)\n",
    "\n",
    "    X_train = train_data.drop(\"bai\", axis=\"columns\")\n",
    "    y_train = train_data[\"bai\"]\n",
    "    X_val = val_data.drop(\"bai\", axis=\"columns\")\n",
    "    y_val = val_data[\"bai\"]\n",
    "    X_test = test_data.drop(\"bai\", axis=\"columns\")\n",
    "    y_test = test_data[\"bai\"]\n",
    "    X_train.shape, y_train.shape, X_test.shape, y_test.shape\n",
    "\n",
    "    # Combina los conjuntos de entrenamiento y validación\n",
    "    X_combined = np.vstack((X_train, X_val))\n",
    "    y_combined = np.hstack((y_train, y_val))\n",
    "\n",
    "    # Define los índices para entrenamiento con -1 y para validación con 0\n",
    "    test_fold = [-1 for _ in range(X_train.shape[0])] + [0 for _ in range(X_val.shape[0])]\n",
    "\n",
    "    # Crea el objeto PredefinedSplit\n",
    "    ps = PredefinedSplit(test_fold)\n",
    "\n",
    "    data_to_save = {}\n",
    "\n",
    "    reg = Ridge()\n",
    "    \n",
    "    # Parametros de grid search\n",
    "    param_grid = {\n",
    "        'alpha': [0.001, 0.01, 0.1, 1, 10],  # Valores de alpha más variados\n",
    "        'max_iter': [100, 200, 300, 400, 500],  # Más iteraciones\n",
    "        'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'saga'],  # Diferentes solvers\n",
    "        'tol': [ 1e-3, 1e-2, 1e-1]  # Tolerancia de la convergencia\n",
    "    }\n",
    "\n",
    "    # Configuración de GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=reg, param_grid=param_grid, cv=ps, scoring='neg_root_mean_squared_error', verbose=3)\n",
    "\n",
    "    # Ajuste de GridSearchCV a tus datos\n",
    "    grid_search.fit(X_combined, y_combined)\n",
    "\n",
    "    # Mejores hiperparámetros encontrados\n",
    "    print(\"Mejores hiperparámetros:\", grid_search.best_params_)\n",
    "\n",
    "    # Guardamos los resultados de los numberBestModels mejores modelos\n",
    "    top_n_results = []\n",
    "\n",
    "    # Obtener los índices de los 10 mejores modelos\n",
    "    top_n_indices = np.argsort(grid_search.cv_results_['rank_test_score'])[:10]\n",
    "\n",
    "    # Iterar sobre cada conjunto de hiperparámetros\n",
    "    for idx in top_n_indices:\n",
    "        \n",
    "        # Extraemos los valores reales de los parámetros (no listas)\n",
    "        params = grid_search.cv_results_['params'][idx]\n",
    "        \n",
    "        # Ajustamos el modelo con los parametros actuales\n",
    "        model = Ridge(random_state=42, **params)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predecir en el conjunto de entrenamiento\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_train_true_denorm = y_train.values * np.array([valorNormalizacion[nt] for nt in X_train[\"nametag_encoded\"].values])\n",
    "        y_train_pred_denorm = y_train_pred * np.array([valorNormalizacion[nt] for nt in X_train[\"nametag_encoded\"].values])\n",
    "\n",
    "        # Calcular métricas en el conjunto de entrenamiento\n",
    "        train_mse = mean_squared_error(y_train_true_denorm, y_train_pred_denorm)\n",
    "        train_rmse = np.sqrt(train_mse)\n",
    "        train_r2 = r2_score(y_train_true_denorm, y_train_pred_denorm)\n",
    "        train_mape = mean_absolute_percentage_error(y_train_true_denorm, y_train_pred_denorm) * 100\n",
    "        \n",
    "        # Predecir en el conjunto de validación\n",
    "        y_val_pred = model.predict(X_val)\n",
    "        y_val_true_denorm = y_val.values * np.array([valorNormalizacion[nt] for nt in X_val[\"nametag_encoded\"].values])\n",
    "        y_val_pred_denorm = y_val_pred * np.array([valorNormalizacion[nt] for nt in X_val[\"nametag_encoded\"].values])\n",
    "        \n",
    "\n",
    "        # Calcular métricas en el conjunto de validación\n",
    "        val_mse = mean_squared_error(y_val_true_denorm, y_val_pred_denorm)\n",
    "        val_rmse = np.sqrt(val_mse)\n",
    "        val_r2 = r2_score(y_val_true_denorm, y_val_pred_denorm)\n",
    "        val_mape = mean_absolute_percentage_error(y_val_true_denorm, y_val_pred_denorm) * 100\n",
    "        \n",
    "        # Predecir en el conjunto de prueba\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        y_test_true_denorm = y_test.values * np.array([valorNormalizacion[nt] for nt in X_test[\"nametag_encoded\"].values])\n",
    "        y_test_pred_denorm = y_test_pred * np.array([valorNormalizacion[nt] for nt in X_test[\"nametag_encoded\"].values])\n",
    "\n",
    "        # Calcular métricas en el conjunto de prueba\n",
    "        test_mse = mean_squared_error(y_test_true_denorm, y_test_pred_denorm)\n",
    "        test_rmse = np.sqrt(test_mse)\n",
    "        test_r2 = r2_score(y_test_true_denorm, y_test_pred_denorm)\n",
    "        test_mape = mean_absolute_percentage_error(y_test_true_denorm, y_test_pred_denorm) * 100\n",
    "        \n",
    "        print(f\"RESULTADOS DE MSE, RMSE, R2, MAPE (Train): {train_mse}, {train_rmse}, {train_r2}, {train_mape}\")\n",
    "        print(f\"RESULTADOS DE MSE, RMSE, R2, MAPE (Val): {val_mse}, {val_rmse}, {val_r2}, {val_mape}\")\n",
    "        print(f\"RESULTADOS DE MSE, RMSE, R2, MAPE (Test): {test_mse}, {test_rmse}, {test_r2}, {test_mape}\")\n",
    "\n",
    "        # Crear un diccionario con toda la información\n",
    "        model_info = {\n",
    "            'params': params,\n",
    "            'MSE_train': train_mse,\n",
    "            'RMSE_train': train_rmse,\n",
    "            'R2_train': train_r2,\n",
    "            'MAPE_train': train_mape,\n",
    "            'MSE_val': val_mse,\n",
    "            'RMSE_val': val_rmse,\n",
    "            'R2_val': val_r2,\n",
    "            'MAPE_val': val_mape,\n",
    "            'MSE_test': test_mse,\n",
    "            'RMSE_test': test_rmse,\n",
    "            'R2_test': test_r2,\n",
    "            'MAPE_test': test_mape\n",
    "        }\n",
    "        \n",
    "        # Agregar al listado de resultados\n",
    "        top_n_results.append(model_info)\n",
    "\n",
    "    # 3. Ordenar los modelos por RMSE de validación\n",
    "    results_list_sorted = sorted(top_n_results, key=lambda x: x['RMSE_val'])\n",
    "\n",
    "    # 4. Obtener los 10 mejores modelos y guardar en un archivo JSON\n",
    "    top_10_models = results_list_sorted[:10]\n",
    "\n",
    "    with open(f\"resultados/LR/{os.path.splitext(archivo)[0]}_best_models_updated_no_train.json\", 'w') as f:\n",
    "        json.dump(top_10_models, f, indent=4)\n",
    "\n",
    "\n",
    "    # Si deseas guardar los 10 mejores modelos\n",
    "    for idx, model_info in enumerate(top_10_models):\n",
    "        params = model_info['params']\n",
    "        model = Ridge(**params)\n",
    "        model.fit(X_train, y_train)\n",
    "        filename = f'resultados/LR/{os.path.splitext(archivo)[0]}_modelo_top_{idx+1}.pkl'\n",
    "        joblib.dump(model, filename)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
