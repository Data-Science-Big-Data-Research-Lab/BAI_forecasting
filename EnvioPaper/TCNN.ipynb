{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ajuste hyperparametros TCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import layers, models\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import os\n",
    "import json\n",
    "\n",
    "import random as python_random\n",
    "\n",
    "from keras_tuner import HyperModel, HyperParameters\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "from keras_tuner.tuners import Hyperband\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "np.random.seed(123)\n",
    "python_random.seed(123)\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (8, 6)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "\n",
    "from funcionesComunes import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCNNModel(HyperModel):\n",
    "\n",
    "    def __init__(self, input_shape, num_TCNN_layers_input=3, num_TCNN_layers_after_input=3):\n",
    "        self.input_shape = input_shape # (SEQ_LENGTH, num_features)\n",
    "        self.num_tcnn_layers_input = num_TCNN_layers_input\n",
    "        self.num_tcnn_layers_after_input = num_TCNN_layers_after_input\n",
    "\n",
    "        # Para controlar las dimensiones temporales\n",
    "        self.temporalDimension = self.input_shape[0]\n",
    "\n",
    "    def build(self, hp):\n",
    "        \n",
    "        # Podemos ver como \"hp.Int\" permite ir modificando los parámetros del modelo con INT\n",
    "        # Con \"hp.Float\" lo hace pero para valores decimales \n",
    "        # IMPORTANTE!!! Tenemos que controlar las dimensionalidad temporal teniendo en cuenta la reducción que se hace en kernel y Pooling\n",
    "        # IMPORTANTE!!! Si estamos aplicando capas LSTM despues Dropout y luego LSTM de nuevo, debemos de poner en todas las primeras capas LSTM return_sequences=True !!!IMPORTANTE\n",
    "        \n",
    "        model = Sequential()\n",
    "        \n",
    "        # Numero de capas de TCNN\n",
    "        num_tcnn_layers_input = hp.Int(\"num_tcnn_layers_input\", min_value = 1, \n",
    "                                       max_value = self.num_tcnn_layers_input, default = self.num_tcnn_layers_input)\n",
    "\n",
    "        for i in range(num_tcnn_layers_input):\n",
    "\n",
    "            # Añadiendo capas TCNN\n",
    "            # Solo la primera capa necesita input_shape\n",
    "\n",
    "            if i == 0:\n",
    "                    \n",
    "                # Primera capa de convolución con padding 'same'\n",
    "                model.add(layers.Conv1D(filters=hp.Int(f'units_tcnn_{i}', min_value=32, max_value=128, step=32),\n",
    "                                        kernel_size=3, activation='relu', \n",
    "                                        padding='same', input_shape=self.input_shape)) # Dimensión temporal despues de la convolución: 4 (3+1) (USAMOS PADDING 'SAME')\n",
    "                model.add(layers.BatchNormalization())\n",
    "                model.add(layers.MaxPooling1D(pool_size=1)) # Reducción de la dimensión temporal a la mitad: 4/2=2 (REDONDEA HACIA ABAJO)\n",
    "\n",
    "                # Obtenemos el redondeo hacia abajo\n",
    "                self.temporalDimension = math.floor(self.temporalDimension / 2)\n",
    "    \n",
    "            else:\n",
    "\n",
    "                # Segunda capa de convolución con padding 'same'\n",
    "                model.add(layers.Conv1D(filters=hp.Int(f'units_tcnn_{i}', min_value=32, max_value=128, step=32),\n",
    "                                        kernel_size=3, activation='relu', \n",
    "                                        padding='same')) # Dimensión temporal despues de la convolución: 2 (USAMOS PADDING 'SAME')\n",
    "                model.add(layers.BatchNormalization())\n",
    "\n",
    "                if self.temporalDimension >= 2:\n",
    "                    model.add(layers.MaxPooling1D(pool_size=1)) # Reducción de la dimensión temporal a la mitad: 2/2=1 (REDONDEA HACIA ABAJO)\n",
    "                    self.temporalDimension = math.floor(self.temporalDimension / 2)\n",
    "                else:\n",
    "                    model.add(layers.MaxPooling1D(pool_size=1)) # Reducción de la dimensión temporal a la mitad: 2/1=2 (REDONDEA HACIA ABAJO)\n",
    "\n",
    "        # Se modifica la capa Dropout desde 0.0 hasta 0.5 con un step de 0.05\n",
    "        model.add(Dropout(hp.Float('dropout', min_value=0.0, max_value=0.5, default=0.25, step=0.05)))\n",
    "        \n",
    "        # Creamos un bucle que permite incrementar las capas LSTM destras de la capa Dropout\n",
    "        num_tcnn_layers_after_input = hp.Int(\"num_tcnn_layers_after_input\", min_value = 1, \n",
    "                                       max_value = self.num_tcnn_layers_after_input, default = self.num_tcnn_layers_after_input)\n",
    "\n",
    "        for i in range(num_tcnn_layers_input):\n",
    "\n",
    "            # Segunda capa de convolución con padding 'same'\n",
    "            model.add(layers.Conv1D(filters=hp.Int(f'units_tcnn_{i}', min_value=32, max_value=128, step=32),\n",
    "                                    kernel_size=3, activation='relu',\n",
    "                                    padding='same')) # USAMOS PADDING 'SAME'\n",
    "            model.add(layers.BatchNormalization())\n",
    "\n",
    "            if self.temporalDimension >= 2:\n",
    "                model.add(layers.MaxPooling1D(pool_size=1)) # REDONDEA HACIA ABAJO\n",
    "                self.temporalDimension = math.floor(self.temporalDimension / 2)\n",
    "            else:\n",
    "                model.add(layers.MaxPooling1D(pool_size=1)) # REDONDEA HACIA ABAJO\n",
    "\n",
    "        # Capa final de convolución con padding 'same'\n",
    "        model.add(layers.Conv1D(filters=hp.Int(f'units_tcnn_{i}', min_value=32, max_value=128, step=32),\n",
    "                                kernel_size=3, activation='relu', padding='same')) # Dimensión temporal despues de la convolución: 1 (USAMOS PADDING 'SAME')\t\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.GlobalAveragePooling1D()) # Promedio global de la dimensión temporal\n",
    "    \n",
    "        # Capa densa\n",
    "        model.add(layers.Dense(hp.Int('dense_units', min_value=4, max_value=64, step=4), \n",
    "                               activation=hp.Choice('dense_activation',values=['relu', 'sigmoid', 'tanh', 'elu', 'relu'])))\n",
    "        \n",
    "        # Capa de salida\n",
    "        model.add(layers.Dense(1, activation=hp.Choice('dense_activation',values=['relu', 'sigmoid', 'tanh', 'elu', 'relu'],default='relu')))\n",
    "        \n",
    "        # Variaciones de los optimizadores\n",
    "        optimizer = hp.Choice('optimizer', ['adam', 'sgd', 'rmsprop'])\n",
    "        \n",
    "        if optimizer == 'adam':\n",
    "            opt = Adam(\n",
    "                learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')\n",
    "            )\n",
    "        elif optimizer == 'sgd':\n",
    "            opt = SGD(\n",
    "                learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')\n",
    "            )\n",
    "        else: # rmsprop\n",
    "            opt = RMSprop(\n",
    "                learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')\n",
    "            )\n",
    "        \n",
    "        \n",
    "        model.compile(optimizer=opt,\n",
    "                        loss=MeanSquaredError(),\n",
    "                        metrics=[RootMeanSquaredError()])\n",
    "            \n",
    "        return model\n",
    "\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(*args,\n",
    "            batch_size=hp.Choice(\"batch_size\", [16, 24, 32]),\n",
    "            **kwargs)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bucle de ajuste de hiperparametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nombreArchivo in os.listdir(\"../Datasets/RCPMerged\"):\n",
    "\n",
    "    print(\"Procesando el archivo: \", nombreArchivo)\n",
    "\n",
    "    # Supongamos que tus datos están en un archivo CSV\n",
    "    df = pd.read_csv(f'../Datasets/RCPMerged/{nombreArchivo}')\n",
    "\n",
    "    # Ver las primeras filas\n",
    "    df = codification(df)\n",
    "\n",
    "    # Normalizamos los datos de crecimiento de los individuos\n",
    "    df, valorNormalizacion = individualNormalization(df)\n",
    "\n",
    "    # División adicional para validación\n",
    "    train_data, val_data, test_data = split_population_individuals(df, train_pct=0.80, val_pct_in_train=0.20, details=False)\n",
    "    train_data.shape, val_data.shape, test_data.shape\n",
    "\n",
    "    WINDOWS_SIZE = 3\n",
    "\n",
    "    # Obtenemos X e y para los datasets de train, val y test \n",
    "    X_train, y_train = df_to_X_y_ind_3(train_data, WINDOWS_SIZE)\n",
    "    X_val, y_val = df_to_X_y_ind_3(val_data, WINDOWS_SIZE)\n",
    "    X_test, y_test = df_to_X_y_ind_3(test_data, WINDOWS_SIZE)\n",
    "    print(X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape)\n",
    "    \n",
    "    # Creación de modelo de ajuste de hiperparámetros\n",
    "    hypermodel = TCNNModel(input_shape=(WINDOWS_SIZE+1, X_train.shape[2]), num_TCNN_layers_input=3, num_TCNN_layers_after_input=3)\n",
    "\n",
    "    tuner = Hyperband(\n",
    "        hypermodel,\n",
    "        objective='val_loss',\n",
    "        max_epochs=50,\n",
    "        factor=3,\n",
    "        directory= 'best_models_TCNN',\n",
    "        project_name=\"resultadosModelosTCNN/\" + nombreArchivo[:-4],\n",
    "    )\n",
    "\n",
    "    print(X_train.shape, X_test.shape, X_val.shape)\n",
    "\n",
    "    # Búsquedas de hiperparámetros\n",
    "    tuner.search(X_train, y_train, epochs=200, validation_data=(X_val, y_val), \n",
    "                    callbacks=[EarlyStopping(monitor='val_loss', patience=10)])\n",
    "\n",
    "    # Obtener los 10 mejores hiperparámetros\n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials=10)\n",
    "\n",
    "    # Obtener los 10 mejores modelos\n",
    "    best_models = tuner.get_best_models(num_models=10)\n",
    "\n",
    "    for i in range(10):\n",
    "        print(f\"Modelo numero {i+1}\\n\")\n",
    "\n",
    "        # Obtener el modelo y los hiperparámetros correspondientes\n",
    "        modelTCNN = best_models[i]\n",
    "        hps = best_hps[i]\n",
    "        batch_size_TCNN = hps.get('batch_size')\n",
    "\n",
    "        # Mostrar el resumen del modelo\n",
    "        print(\"Resumen del modelo:\")\n",
    "        modelTCNN.summary()\n",
    "\n",
    "        # Entrenar el modelo (hacer fit) con los datos de entrenamiento\n",
    "        # Ajustar el número de epochs y callbacks según sea necesario\n",
    "        modelTCNN.fit(X_train, y_train, epochs=200, validation_data=(X_val, y_val), \n",
    "                    callbacks=[EarlyStopping(monitor='val_loss', patience=10)])\n",
    "\n",
    "        # Realizar predicciones y calcular métricas para el conjunto de entrenamiento\n",
    "        predictions_train = predictionForIndividuals(X_train, y_train, modelTCNN, batch_size_TCNN)\n",
    "        predictions_train[\"PredictionsDenormalize\"] = predictions_train.apply(lambda row: desnormalizacionBAI(row, valorNormalizacion, \"Predictions\"), axis=1)\n",
    "        predictions_train[\"ActualDenormalize\"] = predictions_train.apply(lambda row: desnormalizacionBAI(row, valorNormalizacion, \"Actuals\"), axis=1)\n",
    "\n",
    "        train_mse = mean_squared_error(predictions_train[\"ActualDenormalize\"],predictions_train[\"PredictionsDenormalize\"])\n",
    "        train_rmse = np.sqrt(train_mse)\n",
    "        train_mape = (np.sum(np.abs(predictions_train[\"PredictionsDenormalize\"] - predictions_train[\"ActualDenormalize\"])) / np.sum(np.abs(predictions_train[\"ActualDenormalize\"]))) * 100\n",
    "        train_r2 = r2_score(predictions_train[\"ActualDenormalize\"], predictions_train[\"PredictionsDenormalize\"])\n",
    "\n",
    "        # Realizar predicciones y calcular métricas para el conjunto de validación\n",
    "        predictions_val = predictionForIndividuals(X_val, y_val, modelTCNN, batch_size_TCNN)\n",
    "        predictions_val[\"PredictionsDenormalize\"] = predictions_val.apply(lambda row: desnormalizacionBAI(row, valorNormalizacion, \"Predictions\"), axis=1)\n",
    "        predictions_val[\"ActualDenormalize\"] = predictions_val.apply(lambda row: desnormalizacionBAI(row, valorNormalizacion, \"Actuals\"), axis=1)\n",
    "\n",
    "        val_mse = mean_squared_error(predictions_val[\"ActualDenormalize\"],predictions_val[\"PredictionsDenormalize\"])\n",
    "        val_rmse = np.sqrt(val_mse)\n",
    "        val_mape = (np.sum(np.abs(predictions_val[\"PredictionsDenormalize\"] - predictions_val[\"ActualDenormalize\"])) / np.sum(np.abs(predictions_val[\"ActualDenormalize\"]))) * 100\n",
    "        val_r2 = r2_score(predictions_val[\"ActualDenormalize\"], predictions_val[\"PredictionsDenormalize\"])\n",
    "\n",
    "        # Realizar predicciones y calcular métricas para el conjunto de prueba\n",
    "        predictions_test = predictionForIndividuals(X_test, y_test, modelTCNN, batch_size_TCNN)\n",
    "        predictions_test[\"PredictionsDenormalize\"] = predictions_test.apply(lambda row: desnormalizacionBAI(row, valorNormalizacion, \"Predictions\"), axis=1)\n",
    "        predictions_test[\"ActualDenormalize\"] = predictions_test.apply(lambda row: desnormalizacionBAI(row, valorNormalizacion, \"Actuals\"), axis=1)\n",
    "\n",
    "        test_mse = mean_squared_error(predictions_test[\"ActualDenormalize\"],predictions_test[\"PredictionsDenormalize\"])\n",
    "        test_rmse = np.sqrt(test_mse)\n",
    "        test_mape = (np.sum(np.abs(predictions_test[\"PredictionsDenormalize\"] - predictions_test[\"ActualDenormalize\"])) / np.sum(np.abs(predictions_test[\"ActualDenormalize\"]))) * 100\n",
    "        test_r2 = r2_score(predictions_test[\"ActualDenormalize\"], predictions_test[\"PredictionsDenormalize\"])\n",
    "\n",
    "        print(f\"RESULTADOS DE MSE, RMSE, R2, MAPE (Train): {train_mse}, {train_rmse}, {train_r2}, {train_mape}\")\n",
    "        print(f\"RESULTADOS DE MSE, RMSE, R2, MAPE (Val): {val_mse}, {val_rmse}, {val_r2}, {val_mape}\")\n",
    "        print(f\"RESULTADOS DE MSE, RMSE, R2, MAPE (Test): {test_mse}, {test_rmse}, {test_r2}, {test_mape}\")\n",
    "\n",
    "        # Guardar el modelo y los hiperparámetros\n",
    "        modelTCNN.save(f'resultados/TCNNHyperparameter/{nombreArchivo[:-4]}_model_{i+1}.keras')\n",
    "\n",
    "        # Guardar los hiperparámetros y las métricas en un archivo JSON\n",
    "        hps_dict = hps.get_config()['values']\n",
    "        optimizer_config = modelTCNN.optimizer.get_config()\n",
    "        hps_dict.update({\n",
    "            'optimizer_config_type': optimizer_config[\"name\"],\n",
    "            'optimizer_config_learning_rate': float(optimizer_config[\"learning_rate\"]),\n",
    "            'batch_size': batch_size_TCNN,\n",
    "            'mse_train': train_mse,\n",
    "            'rmse_train': train_rmse,\n",
    "            'r2_train': train_r2,\n",
    "            'mape_train': train_mape,\n",
    "            'mse_val': val_mse,\n",
    "            'rmse_val': val_rmse,\n",
    "            'r2_val': val_r2,\n",
    "            'mape_val': val_mape,\n",
    "            'mse_test': test_mse,\n",
    "            'rmse_test': test_rmse,\n",
    "            'r2_test': test_r2,\n",
    "            'mape_test': test_mape\n",
    "        })\n",
    "        \n",
    "        with open(f'resultados/TCNNHyperparameter/{nombreArchivo[:-4]}_model_{i+1}.json', 'w') as f:\n",
    "            json.dump(hps_dict, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
