{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ajuste hyperparametros LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import layers, models\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import os\n",
    "import json\n",
    "\n",
    "import random as python_random\n",
    "\n",
    "from keras_tuner import HyperModel, HyperParameters\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "from keras_tuner.tuners import Hyperband\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.metrics import MeanAbsolutePercentageError\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "np.random.seed(123)\n",
    "python_random.seed(123)\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (8, 6)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "\n",
    "from funcionesComunes import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMHyperModel(HyperModel):\n",
    "    \n",
    "    def __init__(self, input_shape, num_lstm_layers_input=3, num_lstm_layers_after_input=3):\n",
    "        self.input_shape = input_shape\n",
    "        self.num_lstm_layers_input = num_lstm_layers_input\n",
    "        self.num_lstm_layers_after_input = num_lstm_layers_after_input\n",
    "    \n",
    "    def build(self, hp):\n",
    "        \n",
    "        # Podemos ver como \"hp.Int\" permite ir modificando los parámetros del modelo con INT\n",
    "        # Con \"hp.Float\" lo hace pero para valores decimales \n",
    "        # IMPORTANTE!!! Nunca se devuelve el return_sequences si la LSTM es la última capa\n",
    "        # IMPORTANTE!!! Si estamos aplicando capas LSTM despues Dropout y luego LSTM de nuevo, debemos de poner en todas las primeras capas LSTM return_sequences=True !!!IMPORTANTE\n",
    "        \n",
    "        model = Sequential()\n",
    "        \n",
    "        # Número de capas LSTM\n",
    "        num_lstm_layers = hp.Int('num_lstm_layers', min_value=1, max_value=self.num_lstm_layers_input, default = self.num_lstm_layers_input)\n",
    "\n",
    "        for i in range(num_lstm_layers):\n",
    "            # Añadiendo capas LSTM\n",
    "            # Solo la primera capa necesita input_shape\n",
    "            if i == 0:\n",
    "                \n",
    "                # Se tiene una capa LSTM que empieza con 32 unidades y va aumentando de 32 en 32 hasta 128\n",
    "                model.add(LSTM(units=hp.Int(f'units_lstm_{i}', min_value=32, max_value=128, step=32),\n",
    "                               input_shape=self.input_shape,\n",
    "                               return_sequences=True,  # True si hay más de una capa LSTM\n",
    "                               use_bias=True))\n",
    "            else:\n",
    "                model.add(LSTM(units=hp.Int(f'units_lstm_{i}', min_value=32, max_value=128, step=32),\n",
    "                               return_sequences=True,  # True para todas excepto la última capa LSTM\n",
    "                               use_bias=True))\n",
    "        \n",
    "        # Se modifica la capa Dropout desde 0.0 hasta 0.5 con un step de 0.05\n",
    "        model.add(Dropout(hp.Float('dropout', min_value=0.0, max_value=0.5, default=0.25, step=0.05)))\n",
    "        \n",
    "        # Creamos un bucle que permite incrementar las capas LSTM destras de la capa Dropout\n",
    "        num_lstm_layers_after = hp.Int('num_lstm_layers_after', min_value=1, max_value=self.num_lstm_layers_after_input, default = self.num_lstm_layers_after_input)\n",
    "        \n",
    "        for i in range(num_lstm_layers_after):\n",
    "            # Añadiendo capas LSTM después de Dropout\n",
    "            model.add(LSTM(units=hp.Int(f'units_lstm_after_{i}', min_value=32, max_value=128, step=32),\n",
    "                           return_sequences=(i < num_lstm_layers_after - 1),  # True para todas excepto la última capa LSTM\n",
    "                           use_bias=True))\n",
    "        \n",
    "        # Capa dense que empieza con 4 unidades y va aumentando de 4 en 4 hasta 64\n",
    "        model.add(Dense(hp.Int('dense_units', min_value=4, max_value=64, step=4), activation=hp.Choice('dense_activation',values=['relu', 'sigmoid', 'tanh', 'elu', 'relu'],default='relu'), use_bias=True))\n",
    "        \n",
    "        # Se modifica la capa Dropout desde 0.0 hasta 0.5 con un step de 0.05\n",
    "        model.add(Dropout(hp.Float('dropout_2', min_value=0.0, max_value=0.5, default=0.25, step=0.05)))\n",
    "\n",
    "        model.add(Dense(1, activation=hp.Choice('dense_activation',values=['relu', 'sigmoid', 'tanh', 'elu', 'relu'],default='relu'), use_bias=True))\n",
    "        \n",
    "        # Variaciones de los optimizadores\n",
    "        optimizer = hp.Choice('optimizer', ['adam', 'sgd', 'rmsprop'])\n",
    "        if optimizer == 'adam':\n",
    "            opt = Adam(\n",
    "                learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')\n",
    "            )\n",
    "        elif optimizer == 'sgd':\n",
    "            opt = SGD(\n",
    "                learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')\n",
    "            )\n",
    "        else: # rmsprop\n",
    "            opt = RMSprop(\n",
    "                learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')\n",
    "            )\n",
    "        \n",
    "        \n",
    "        model.compile(optimizer=opt,\n",
    "                      loss=MeanSquaredError(),\n",
    "                      metrics=[\n",
    "                        MeanSquaredError(name='mse'),\n",
    "                        RootMeanSquaredError(name='rmse'),\n",
    "                        MeanAbsolutePercentageError(name='mape')\n",
    "                        ]\n",
    "                    )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(*args,\n",
    "            batch_size=hp.Choice(\"batch_size\", [16, 24, 32]),\n",
    "            **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bucle de ajuste de hiperparametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nombreArchivo in os.listdir(\"../Datasets/RCPMerged\"):\n",
    "\n",
    "    print(\"Procesando el archivo: \", nombreArchivo)\n",
    "\n",
    "    # Supongamos que tus datos están en un archivo CSV\n",
    "    df = pd.read_csv(f'../Datasets/RCPMerged/{nombreArchivo}')\n",
    "\n",
    "    # Ver las primeras filas\n",
    "    df = codification(df)\n",
    "\n",
    "    # Normalizamos los datos de crecimiento de los individuos\n",
    "    df, valorNormalizacion = individualNormalization(df)\n",
    "\n",
    "    # División adicional para validación\n",
    "    train_data, val_data, test_data = split_population_individuals(df, train_pct=0.80, val_pct_in_train=0.20, details=False)\n",
    "    train_data.shape, val_data.shape, test_data.shape\n",
    "\n",
    "    WINDOWS_SIZE = 3\n",
    "\n",
    "    # Obtenemos X e y para los datasets de train, val y test \n",
    "    X_train, y_train = df_to_X_y_ind_3(train_data, WINDOWS_SIZE)\n",
    "    X_val, y_val = df_to_X_y_ind_3(val_data, WINDOWS_SIZE)\n",
    "    X_test, y_test = df_to_X_y_ind_3(test_data, WINDOWS_SIZE)\n",
    "    print(X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "    # Creación de modelo de ajuste de hiperparámetros\n",
    "    hypermodel = LSTMHyperModel(input_shape=(WINDOWS_SIZE+1, X_train.shape[2]), num_lstm_layers_input=3, num_lstm_layers_after_input=3)\n",
    "\n",
    "    tuner = Hyperband(\n",
    "        hypermodel,\n",
    "        objective='val_loss',\n",
    "        max_epochs=50,\n",
    "        factor=3,\n",
    "        directory= 'resultadosModelosLSTM',\n",
    "        project_name=\"resultadosModelosLSTM/\" + nombreArchivo[:-4],\n",
    "    )\n",
    "    \n",
    "    print(X_train.shape, X_test.shape, X_val.shape)\n",
    "\n",
    "    # Búsquedas de hiperparámetros\n",
    "    tuner.search(X_train, y_train, epochs=200, validation_data=(X_val, y_val), \n",
    "                    callbacks=[EarlyStopping(monitor='val_loss', patience=10)])\n",
    "\n",
    "    # Obtener los 10 mejores hiperparámetros\n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials=10)\n",
    "\n",
    "    # Obtener los 10 mejores modelos\n",
    "    best_models = tuner.get_best_models(num_models=10)\n",
    "\n",
    "    for i, trial in enumerate(best_models):\n",
    "        print(f\"Modelo numero {i+1}\\n\")\n",
    "\n",
    "        # Obtener el modelo y los hiperparámetros correspondientes\n",
    "        model = best_models[i]\n",
    "        hps = best_hps[i]\n",
    "        batch_size_LSTM = hps.get('batch_size')\n",
    "\n",
    "        # Mostrar el resumen del modelo\n",
    "        print(\"Resumen del modelo:\")\n",
    "        model.summary()\n",
    "\n",
    "        # Entrenar el modelo (hacer fit) con los datos de entrenamiento\n",
    "        # Ajustar el número de epochs y callbacks según sea necesario\n",
    "        model.fit(X_train, y_train, epochs=2, validation_data=(X_val, y_val), \n",
    "                    callbacks=[EarlyStopping(monitor='val_loss', patience=10)])\n",
    "\n",
    "        # Entrenamiento\n",
    "        predictions_train = predictionForIndividuals(X_train, y_train, model, batch_size_LSTM)\n",
    "        predictions_train[\"PredictionsDenormalize\"] = predictions_train.apply(lambda row: desnormalizacionBAI(row, valorNormalizacion, \"Predictions\"), axis=1)\n",
    "        predictions_train[\"ActualDenormalize\"] = predictions_train.apply(lambda row: desnormalizacionBAI(row, valorNormalizacion, \"Actuals\"), axis=1)\n",
    "\n",
    "        train_mse = mean_squared_error(predictions_train[\"ActualDenormalize\"], predictions_train[\"PredictionsDenormalize\"])\n",
    "        train_rmse = np.sqrt(train_mse)\n",
    "        train_mape = mean_absolute_percentage_error(predictions_train[\"ActualDenormalize\"], predictions_train[\"PredictionsDenormalize\"]) *100\n",
    "        train_r2 = r2_score(predictions_train[\"ActualDenormalize\"], predictions_train[\"PredictionsDenormalize\"])\n",
    "\n",
    "        # Validación\n",
    "        predictions_val = predictionForIndividuals(X_val, y_val, model, batch_size_LSTM)\n",
    "        predictions_val[\"PredictionsDenormalize\"] = predictions_val.apply(lambda row: desnormalizacionBAI(row, valorNormalizacion, \"Predictions\"), axis=1)\n",
    "        predictions_val[\"ActualDenormalize\"] = predictions_val.apply(lambda row: desnormalizacionBAI(row, valorNormalizacion, \"Actuals\"), axis=1)\n",
    "\n",
    "        val_mse = mean_squared_error(predictions_val[\"ActualDenormalize\"], predictions_val[\"PredictionsDenormalize\"])\n",
    "        val_rmse = np.sqrt(val_mse)\n",
    "        val_mape = mean_absolute_percentage_error(predictions_val[\"ActualDenormalize\"], predictions_val[\"PredictionsDenormalize\"]) *100\n",
    "        val_r2 = r2_score(predictions_val[\"ActualDenormalize\"], predictions_val[\"PredictionsDenormalize\"])\n",
    "\n",
    "        # Prueba\n",
    "        predictions_test = predictionForIndividuals(X_test, y_test, model, batch_size_LSTM)\n",
    "        predictions_test[\"PredictionsDenormalize\"] = predictions_test.apply(lambda row: desnormalizacionBAI(row, valorNormalizacion, \"Predictions\"), axis=1)\n",
    "        predictions_test[\"ActualDenormalize\"] = predictions_test.apply(lambda row: desnormalizacionBAI(row, valorNormalizacion, \"Actuals\"), axis=1)\n",
    "\n",
    "        test_mse = mean_squared_error(predictions_test[\"ActualDenormalize\"], predictions_test[\"PredictionsDenormalize\"])\n",
    "        test_rmse = np.sqrt(test_mse)\n",
    "        test_mape = mean_absolute_percentage_error(predictions_test[\"ActualDenormalize\"], predictions_test[\"PredictionsDenormalize\"]) *100\n",
    "        test_r2 = r2_score(predictions_test[\"ActualDenormalize\"], predictions_test[\"PredictionsDenormalize\"])\n",
    "\n",
    "        print(f\"RESULTADOS DE MSE, RMSE, R2, MAPE (Train): {train_mse}, {train_rmse}, {train_r2}, {train_mape}\")\n",
    "        print(f\"RESULTADOS DE MSE, RMSE, R2, MAPE (Val): {val_mse}, {val_rmse}, {val_r2}, {val_mape}\")\n",
    "        print(f\"RESULTADOS DE MSE, RMSE, R2, MAPE (Test): {test_mse}, {test_rmse}, {test_r2}, {test_mape}\")\n",
    "\n",
    "        # Guardar el modelo y los hiperparámetros\n",
    "        model.save(f'resultados/LSTMHyperparameter/{nombreArchivo[:-4]}_model_{i+1}.keras')\n",
    "\n",
    "        # Guardar los hiperparámetros y las métricas en un archivo JSON\n",
    "        hps_dict = hps.get_config()['values']\n",
    "        optimizer_config = model.optimizer.get_config()\n",
    "        hps_dict.update({\n",
    "            'optimizer_config_type': optimizer_config[\"name\"],\n",
    "            'optimizer_config_learning_rate': float(optimizer_config[\"learning_rate\"]),\n",
    "            'batch_size': batch_size_LSTM,\n",
    "            'mse_train': train_mse,\n",
    "            'rmse_train': train_rmse,\n",
    "            'r2_train': train_r2,\n",
    "            'mape_train': train_mape,\n",
    "            'mse_val': val_mse,\n",
    "            'rmse_val': val_rmse,\n",
    "            'r2_val': val_r2,\n",
    "            'mape_val': val_mape,\n",
    "            'mse_test': test_mse,\n",
    "            'rmse_test': test_rmse,\n",
    "            'r2_test': test_r2,\n",
    "            'mape_test': test_mape\n",
    "        })\n",
    "\n",
    "    # Cargar el archivo JSON existente\n",
    "    with open(f'resultados/LSTMHyperparameter/{nombreArchivo[:-4]}_model_{i+1}.json', 'w') as f:\n",
    "        json.dump(hps_dict, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
